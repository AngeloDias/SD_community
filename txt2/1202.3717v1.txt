Csaba Szepesvári
Department of Computing Science
University of Alberta
Edmonton, Canada
szepesva@ualberta.ca

1984). These bounds, however, are often loose and impractical, reflecting the inherent difficulty of the learning problem when no assumptions are made on the
distribution of the data.
Both Bayesian and PAC methods have been proposed separately for reinforcement learning (Kearns
and Singh, 2002; Brafman and Tennenholtz, 2003;
Strehl and Littman, 2005; Kakade, 2003; Duff, 2002;
Wang et al., 2005; Poupart et al., 2006; Kolter and
Ng, 2009), where an agent is learning to interact with
an environment to maximize some objective function.
These methods are mostly focused on the so-called
exploration–exploitation problem, where one aims to
balance the amount of time spent on gathering information about the dynamics of the environment and the
time spent acting optimally according to the current
estimates. PAC methods are much more conservative
and spend more time exploring the system and collecting information. Bayesian methods, on the other
hand, are greedier and only solve the problem over a
limited planning horizon.
The PAC-Bayesian approach (McAllester, 1999;
Shawe-Taylor and Williamson, 1997), takes the best
of both worlds by combining the distribution-free correctness of PAC theorems with the data-efficiency of
Bayesian inference. PAC-Bayesian bounds do not require the Bayesian assumption to hold. They instead
measure the consistency of the prior over the training data, and leverage the prior only when it seems
informative. The empirical results of model selection
algorithms for classification tasks using these bounds
are comparable to some of the most popular learning algorithms, such as AdaBoost and Support Vector
Machines (Germain et al., 2009).
Fard and Pineau (2010) introduced the idea of PACBayesian model-selection in reinforcement learning
(RL) for finite state spaces. They provided PACBayesian bounds on the approximation error in the
value function of stochastic policies when a prior distribution is available either on the space of possible

models, or on the space of value functions. Model selection based on these bounds provides a robust use of
Bayesian priors outside the Bayesian inference framework. Their work, however, is limited to small and
discrete domains, and is mostly useful when sample
transitions are drawn uniformly across the state space.
This is problematic as most RL domains are relativity
large, and require function approximation over continuous state spaces.
This paper provides the first PAC-Bayesian bound
for value function approximation on continuous state
spaces. We use results by Samson (2000) to handle
non i.i.d. data that are collected on Markovian processes, and use this, along with general PAC-Bayesian
inequalities, to get a bound on the approximation error
of a value function sampled from any distribution over
measurable functions. We empirically evaluate these
bounds for model selection on two different continuous RL domains in a knowledge-transfer setting. Our
results show that a PAC-Bayesian approach in this setting is indeed able to use the prior distribution when
it is informative and matches the data, and ignore it
when it is misleading.

2

the transition and reward models are not known, one
can use a finite sample set of transitions to learn an
approximate value function. Least-squares temporal
difference learning (LSTD) and its derivations (Boyan,
2002; Lagoudakis and Parr, 2003) are among the methods used to learn a value function based on a finite
sample.

3

A general PAC-Bayes bound

We begin by first stating a general PAC-Bayes bound.
In the next section, we use this result to derive our
main bound for the approximation error in an RL setting.
Let F be a class of real-valued functions over a wellbehaved domain X (i.e., X could be a bounded measurable subset of a Euclidean space). For ease of
presentation, we assume that F has countably many
functions. For a measure ρ over F, and a functional,
def R
R : F → R, we define ρR = R(f )dρ(f ).
Theorem 1. Let R be a random functional over F
with a bounded range. Assume that for some C > 0,
c > 1, for any 0 < δ < 1 and f ∈ F, w.p. 1 − δ,

Background and Notation
r

A Markov Decision Process (MDP) M = (X , A, T, R)
is defined by a (possibly infinite) set of states X ,
a set of actions A, a transition probability kernel
T : X × A → M(X ), where T (.|x, a) defines the distribution of next state given that action a is taken in
state x, and a (possibly stochastic) reward function
R : X × A → M([0, Rmax ]). Throughout the paper,
we focus on finite-action, continuous state, discountedreward MDPs, with the discount factor denoted by
γ ∈ [0, 1). At discrete time steps, the reinforcement
learning agent chooses an action and receives a reward.
The environment then changes to a new state according to the transition kernel.
A policy is a (possibly stochastic) function from states
to actions. The value of a state x for policy π, denoted
the expected value of the discounted sum
by V π (x), isP
of rewards ( t γ t rt ) if the agent starts in state x and
acts according to policy π. The value function satisfies
the Bellman equation:
Z
π
V (x) = R(x, π(x)) + γ V π (y)T (dy|x, π(x)). (1)
There are many methods developed to find the value of
a policy (policy evaluation) when the transition and reward functions are known. Among these there are dynamic programming methods in which one iteratively
applies the Bellman operator (Sutton and Barto, 1998)
to an initial guess of the optimal value function. When

R(f ) ≤

log(C/δ)
.
c

(2)

Then, for any measure ρ0 over F, w.p. 1 − δ, for all
measures ρ over F:
s
ρR ≤

log( 1+C(c−1)
) + K(ρ, ρ0 )
δ
,
c−1

(3)

where K(ρ, ρ0 ) denotes the Kullback-Leibler divergence
between ρ and ρ0 .
The proof (included in the appendix) is a straightforward generalization of the proof presented by
Boucheron et al. (2005).

4

Application to RL

Consider some MDP, with state space X , and a policy
π whose stationary distribution ρπ exists. Let Dn =
0
((Xi , Ri , Xi+1
)ni=1 ) be a random sample of size n such
π
0
that Xi ∼ ρ , (Xi+1
, Ri ) ∼ P π (·|Xi ), where P π is the
Markov kernel underlying policy π: The ith datum
0
(Xi , Ri , Xi+1
) is an elementary transition from state
0
Xi to state Xi+1
while policy π is followed and the
reward associated with the transition is Ri . Further, to
simplify the exposition, let (X, R, X 0 ) be a transition
whose joint distribution is the same as the common
0
joint of (Xi , Ri , Xi+1
).

Define the functionals R, Rn over the space of realvalued, bounded measurable functions over X as follows: Let V : X → R be such a function. Then
R(V )
Rn (V )

= E
=

n
o2 
R + γV (X 0 ) − V (X)
,

2

(Antos et al., 2008)).1 Thus, for ε2π (V ) = kV − V π kρπ ,
ε2π (V ) ≤

1
[R(V ) − Γπ (V )] .
(1 − γ)2

(7)

Combining this inequality with (5), we get the following result:

n
o2
1 Xn
Ri + γV (Xi0 ) − V (Xi ) .
n i=1

The functional R is called the squared sample Bellman error, while Rn is the empirical squared sample
Bellman error. Clearly, E [Rn (V )] = R(V ) holds. The
following lemma (proved in the appendix) is a concentration bound connecting R and Rn .
Lemma 2. Under proper mixing conditions for the
sample, and assuming that the random rewards are
sub-Gaussian, there exists constants c1 > 0, c2 ≥ 1
which depend only on P π such that for any Vmax > 0,
for any measurable function V bounded by Vmax , and
any 0 < δ < 1, w.p. 1 − δ,

Theorem 3. Fix a countable set F of real-valued,
measurable functions with domain X , which are
bounded by Vmax . Assume that the conditions of
Lemma 2 hold and let c1 , c2 be as in this lemma. Fix
any measure µ0 over these functions. Assume that
2
n > Vmax
c1 . Then, for all 0 < δ < 1, with probability
1 − δ, for all measures µ over F:
v


u
(
u log
c2 n
+ K(µ, µ0 )
2
u
c
V
δ
1
1 max
t
µR
+
µε2π ≤
n
n
(1 − γ)2
2
Vmax
c1 − 1
)
− µ E [Γπ ]

.
2

r
R(V ) − Rn (V ) ≤

c 
2 c
Vmax
1
2
log
.
n
δ

(4)

Hence, by Theorem 1, for any countable set F of functions V bounded by Vmax , for any distribution µ0 over
2
c1 , then for all 0 < δ < 1,
these functions, if n > Vmax
w.p. 1 − δ, for all measures µ over F:
v


u
u log
c2 n
2
u
c1 Vmax
δ + K(µ, µ0 )
µ(R − Rn ) ≤ t
.
n
V 2 c1 − 1

(5)

Further, the same bound holds for V̄µ − V π ρπ , where
R
V̄µ = V dµ(V ) is the µ-average of value functions
from F.
Proof. The first statement follows from (7) combined
with (5), as noted earlier. To see this just replace R(V ) in (7) with R(V ) − Rn (V ) + Rn (V ).
Then, integrate both sides with respect to µ and apply (5) to bound µ(R − Rn ). The second part follows from the first part, Fubini’s theorem and Jensen’s
R π
2
2
(V − V )dµ(V ) ρπ ≤
inequality: V̄µ − V π ρπ =
R
2
kV π − V kρπ dµ(V ) = µε2π .

max

Now, we show how this bound can be used to derive a
PAC-Bayes bound on the error of a value function V
that is drawn from an arbitrary distribution over measurable functions. For a distribution ρ over the state
L2 norm: kV k2ρ =
Rspace X2, let k · kρ be the weighted
π
[V (x)] dρ(x). Further, let B be the Bellman operator underlying π: B π V (x) = E [R + γV (X 0 )|X = x].
Fix some V . Since B π is a γ-contraction w.r.t. the
norm k · kρπ , a standard argument shows that (Bertsekas and Tsitsiklis, 1996):

kV − V π kρπ ≤

kB π V − V kρπ
1−γ

.

(6)

Now,
variance decomposition Var [U ] =
 using the
2
E U 2 −E [U ] , we get R(V ) = kB π V −V k2ρπ +Γπ (V ),
where Γπ (V ) = E [Var [R + γV (X 0 )|X]] (see, e.g.,

The theorem bounds the expected error of approximating V π with a value function drawn randomly from
some distribution µ. Note that in this theorem, µ0
must be a fixed distribution, chosen a priori (i.e. prior
distribution), but µ can be chosen in a data dependent
manner, i.e., it can be a “posterior” distribution.
Notice that there are three elements to the above
bound (right hand side). The first term is the empirical component of the bound, which enforces the
selection of solutions with smaller empirical Bellman
residuals. The second term is the Bayesian component of the bound, which penalizes distributions that
are far from the prior. The third term corrects for the
variance in the return at each state.
1



Here, Var [U |V ] = E (U − E [U |V ])2 |V is the conditional variance of U as usual. We shall also use the similarly
defined conditional covariance, Cov [(U1 , U2 )|V ] =

E (U1 − E [U1 |V ])(U2 − E [U2 |V ])> |V . When U1 = U2 ,
def

we will also use Cov [U1 |V ] = Cov [(U1 , U2 )|V ].

If we can empirically estimate the right hand side of
the above inequality, then we can use the bound in
an algorithm. For example, we can derive a PACBayesian model-selection algorithm that searches in
the space of posteriors µ so as to minimize the upper
bound.
4.1

Linearly parametrized classes of
functions

Theorem 3 is presented for any countable families of
functions. One can extend this result to sufficiently
regular classes of functions (which can carry measures)
without any problems.2 Here we consider the case
where F is the class of linearly parametrized functions
with bounded parameters,

FC = θ> φ : kθk ≤ C
where φ : X → Rd is some measurable function such
def
that Fmax = supx∈X kφ(x)k2 < ∞. In this case, the
measures can be put on the ball { θ : kθk ≤ C }.
Let us now turn to the estimation of the variance term.
Assuming that the reward for each transition is independent of the next state, one gets
Var [R + γV (X 0 )|X]

= Var [R|X]
+γ 2 Var [V (X 0 )|X] .

Now, if Vθ = φ> θ, then:
Var [V (X 0 )|X]

= θ> Cov [φ(X 0 )|X] θ.

Assuming homoscedastic variance for the rewards, and
2
= Var [R] and Σφ = E [Cov [φ(X 0 )|X]], we
defining σR
get:
2
Var [R + γV (X 0 )|X] = σR
+ γ 2 θ > Σφ θ .

4.2

Estimating the constants

2
In some cases the terms σR
and Σφ are known (e.g.,
2
σR = 0 when the rewards are a deterministic function
of the start state and action, and Σφ = 0 when the
dynamics is deterministic). An alternative is to estimate these terms empirically. This can be done by,
e.g., double sampling of next states (assuming one has
access to a generative model, or if one can reset the
state).3 If such estimates are generated based on finite
sample sets, then we might need to add extra deviation terms to the bound of Theorem 3. For simplicity,
2

The extension presents only technical challenges, but
leaves the result intact and hence is omitted.
3
Alternately, one can co-estimate the mean and variance terms (Sutton et al., 2009), keeping a current guess of
them and updating both estimates as new transitions are
observed.

we assume that these terms are either known or can be
estimated on a separate dataset of many transitions.
Examples of such cases are studied in the empirical
results.
The constant c2 , which depends on the mixing condition of the process, can also be estimated if we have
access to a generative model. There are upper bounds
for c2 when the sample is a collection of independent
trajectories of length less than h.

5

Empirical Results

In this section, we investigate how the bound of Theorem 3 can be used in a model selection mechanism for
transfer learning in the RL setting. One experiment
is presented on the well-known mountain car problem,
the other focuses on a generative model of epileptic
seizures built from real-world data.
5.1

Case Study: Mountain Car

We design a transfer learning experiment on the Mountain Car domain (Sutton and Barto, 1998), where the
goal is to drive an underpowered car beyond a certain altitude up a mountain. We refer the reader to
the reference for details of the domain. We learn the
optimal policy (name it π) on the original Mountain
Car problem (γ = 0.9, reward = 1 passed the goal
threshold and 0 otherwise). Note that the reward and
2
= 0 and
the dynamics are deterministic, therefore σR
Σφ = 0. The task is to learn the value function on
the original domain, and use that knowledge in similar (though not identical) environments to accelerate
the learning process in those new environments. (The
other environments will be described later.)
We estimate the value of π on the original domain with
tile coding (4 tiles of size 8 × 8). Let θ0 be the LSTD
solution on a very large sample set in the original domain. To transfer the domain knowledge from this
problem, we construct a prior distribution µ0 : product of Gaussians with mean θ0 and variance σ02 = 0.01.
In a new environment, we collect a set of trajectories
(100 trajectories of length 5), and search in the space of
λ-parametrized posterior measures, defined as follows:
measure µλ is the product of Gaussians with mean
. 


−1

λθ0
θ̂
λ
1
λ
1
+
+
and
variance
+
,
2
2
2
2
2
2
σ̂
σ̂
σ̂
σ
σ
σ
0

0

0

where θ̂ is the LSTD solution based on the sample set
on the new environment, and σ̂ 2 (variance of the empirical estimate) is set to 0.01. The search for the best
λ-parameterized posterior is driven by our proposed
PAC-Bayes upper bound on the approximation error.
When λ = 0, µλ will be a purely empirical estimate,
whereas when λ = 1, we get the Bayesian posterior for

We test this model-selection method on two new environments. The first is a mountain domain very similar
to the original problem, where we double the effect
of the acceleration of the car. The true value function of this domain is close the original domain, and
so we expect the prior to be informative (and thus λ
to be close to 1). In the second domain, we change
the reward function such that it decreases, inversely
proportional to the car’s altitude: r(x) = 1 − h(x),
where h(x) ∈ [0, 1] is the normalized altitude at state
x. The value function of π under this reward function
is largely different from that of the original one, which
means that the prior distribution is misleading, and
the empirical estimate should be more reliable (and λ
close to 0).
Table 1 reports the average true error of approximating V π using different methods over 100 runs (purely
empirical method is when λ = 0, Bayesian is when
λ = 1). This corresponds to the left hand side of Theorem 3 for these methods. For the similar environment, the PAC-Bayes bound is minimized consistently
with λ = 1, indicating that the method is fully using the Bayesian prior. The error is thus decreased to
less than a half of that of the empirical estimate. For
the environment with largely different reward function,
standard Bayesian inference results in poor approximation, whereas the PAC-Bayes method is selecting
small values of λ and is mostly ignoring the prior.
Table
1: Error in the estimated value function V π
R
2
( kV − V π kρπ dµ(V )) on the Mountain Car domain.
The last row shows the value of the λ parameter selected by the PAC-Bayesian method.
Purely empirical
Bayesian
PAC-Bayes
λPAC−Bayes

Similar Env

Different Env

2.35 ± 0.12
1.03 ± 0.09
1.03 ± 0.09
1

0.03 ± 0.01
2.38 ± 0.05
0.07 ± 0.01
0.06 ± 0.01

To further investigate how the value function estimate
changes with these different methods, we consider an
estimate of the value for the state when the car is at
the bottom of the hill. This point estimate is constructed from the PAC-Bayes estimate using the value
function obtained by using only the mean of µλ . To

Figure 5.1(right) compares the distribution of the estimated values for the highly different environment.
We can see that, as expected, the Bayesian estimate
is heavily biased due to the use of a misleading prior.
The PAC-Bayes estimate is only slightly biased away
from the empirical one with the same variance on the
value. Again, this confirms that PAC-Bayes modelselection is largely ignoring the prior when the prior is
misleading.
35
40
30

Emp

30

Bayes

PacBayes

25

Emp

frequency

Note that because the Mountain Car is a deterministic
domain, the variance term of Theorem 3 is 0. As we
use trajectories with known length, we can also bound
the other constants in the bound and evaluate the
bound completely empirically based on the observed
sample set.

get a sense of the dependence of this estimate on the
randomness of the sample the estimate is constructed
over 100 runs. We also obtain these estimates using
a Bayes estimate and purely empirical estimate. Figure 5.1(left) shows a normal fit to the histogram of the
resulting estimates, for the purely empirical and the
PAC-Bayes estimates. As it can be seen, the distribution of PAC-Bayes estimates (which coincides with
the Bayesian posterior as the best λ is consistently 1
in this case) is centered around the correct value, but
is more peaked than the empirical distribution. This
shows that the method is using the prior to converge
faster to the correct value.

frequency

the mean of a Gaussian with known variance (standard
Bayesian inference with empirical priors).

20

PacBayes

20
15
10

10

5
0
−4

−2

0

2

4

6

0

0.5

value

1
value

1.5

Figure 1: Distribution of the estimated value function
on similar (left) and different (right) environments
5.2

Case Study: Epilepsy Domain

We also evaluate our method on a more complex domain. The goal of the RL agent here is to apply
direct electrical neurostimulation such as to suppress
epileptiform behavior. We use a generative model constructed from real-world data collected on slices of rat
brain tissues (Bush et al., 2009); the model is available
in the RL-Glue framework. Observations are generated over a 4-dimensional real-valued state space. The
action choice corresponds to selecting the frequency at
which neurostimulation is applied. The reward is −1
for steps when a seizure is occurring, −1/40 for each
stimulation pulse, and 0 otherwise.
We first apply the best clinical fixed rate policy (stimulation is applied at a consistent 1Hz) to collect a large
sample set (Bush et al., 2009). We then use LSTD to
learn a linear value function over the original feature
space. Similar to the experiment described above, we
construct a prior (with a similar mean and variance
structure), and use it for knowledge transfer in two

new cases. This time, we keep the dynamics and reward function intact and instead change the policy.
The first modified policy we consider applies stimulation at a fixed rate of 2Hz; this is expected to have
a similar value function as the original (1Hz) policy.
The other policy we consider applies no stimulation;
this is expected to have a very different value function
as the seizures are not suppressed.
Table
2: The error of value-function estimates
R
2
( kV − V π kρπ dµ(V )) on the Epilepsy domain.
Empirical
Bayesian
PAC-Bayes
λPAC−Bayes

2 Hz Stimulation

No Stimulation

0.0044 ± 0.0007
0.0013 ± 0.0001
0.0022 ± 0.0004
0.62 ± 0.05

0.54 ± 0.06
0.86 ± 0.07
0.69 ± 0.08
0.30 ± 0.05

We sample 10,000 on-policy trajectories of length 1
and use them with the PAC-Bayes model-selection
mechanism described previously (with similar λparametrized posterior family on the θ parameters,
γ = 0.8) to get estimates of the value function. Table 2 summarizes the performance of different methods
on the evaluation of the new policies (averaged over
50 runs). The results are not as polarized as those
of the Mountain Car experiment, partly because the
domain is noisier, and because the prior is neither exclusively informative or misleading. Nonetheless, we
observe that the PAC-Bayes method is using the prior
more (λ averaging around 0.62) in the case of the 2Hz
policy, which is consistent with clinical evidence showing that 1Hz and 2Hz have similar effect (Bush et al.,
2009), whereas the prior is considered less (λ averaging
around 0.30) in the case of the 0Hz policy which has
substantially (though not entirely) different effects.

6

Discussion

This paper introduces the first PAC-Bayesian bound
for policy evaluation with function approximation and
general state spaces. We demonstrate how such
bounds can be used for value function estimation based
on finite sample sets. Our empirical results show that
PAC-Bayesian model-selection uses prior distributions
when they are informative, and ignores them when
they are misleading. Our results thus far focus on the
policy evaluation case. This approach can be used in
a number of applications, including transfer learning,
as explored above.
Model-selection based on error bounds has been studied previously with regularization techniques (Farahmand et al., 2009). These bounds are generally tighter
for point estimates, as compared to the distributions

used in this work. However, our method is more general as it could incorporate arbitrary domain knowledge into the learning algorithm with any type of prior
distribution. It can of course use sparsity or smoothness priors, which correspond to well-known regularization methods.
An alternative is to derive margin bounds, similar
to those of large-margin classifiers, using PAC-Bayes
techniques. This was recently done by Fard and
Pineau (2010) in the discrete case. The extension to
continuous domains with general function approximation is an interesting future work.
This work does not address the application of PACBayes bounds to derive exploration strategies for the
RL problem. Seldin et al. (2011b,a) have studied
the exploration problem for multiarmed bandits and
have provided algorithms based on PAC-Bayes analysis of martingales. Extensions to contextual bandits
and more general RL settings remain interesting open
problems.

Acknowledgements
This work was supported in part by AICML, AITF
(formerly iCore and AIF), the PASCAL2 Network of
Excellence under EC (grant no. 216886), the NSERC
Discovery Grant program and the National Institutes
of Health (grant R21 DA019800).

References
A. Antos, Cs. Szepesvári, and R. Munos. Learning
near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample
path. Machine Learning, 71(1):89–129, April 2008.
D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming (Optimization and Neural Computation Series, 3). Athena Scientific, 1996. ISBN 1886529108.
S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classification: A survey of some recent advances. ESAIM:
Probability and Statistics, 9:323–375, 2005.
J. A. Boyan. Technical update: Least-squares temporal
difference learning. Machine Learning, 49(2):233–246,
2002.
R. I. Brafman and M. Tennenholtz. R-max – A general
polynomial time algorithm for near-optimal reinforcement learning. The Journal of Machine Learning Research, 3:213–231, 2003.
K. Bush, J. Pineau, A. Guez, B. Vincent, G. Panuccio, and
M. Avoli. Dynamic Representations for Adaptive Neurostimulation Treatment of Epilepsy. 4th International
Workshop on Seizure Prediction, 2009.
M. O. G. Duff. Optimal learning: Computational procedures for Bayes-adaptive Markov decision processes.
PhD thesis, University of Massachusetts Amherst, 2002.
A. M. Farahmand, M. Ghavamzadeh, Cs. Szepesvári, and
S. Mannor. Regularized policy iteration. In NIPS, 2009.

M. M. Fard and J. Pineau. PAC-Bayesian model selection
for reinforcement learning. In NIPS, 2010.
P. Germain, A. Lacasse, F. Laviolette, and M. Marchand.
PAC-Bayesian learning of linear classifiers. In ICML,
2009.
S. M. Kakade. On the sample complexity of reinforcement
learning. PhD thesis, University College London, 2003.
M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2-3):209–
232, 2002.
J. Z. Kolter and A. Y. Ng. Near-Bayesian exploration in
polynomial time. In ICML, pages 513–520, 2009.
M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–
1149, 2003. ISSN 1532-4435.
D. A. McAllester. Some PAC-Bayesian theorems. Machine
Learning, 37(3):355–363, 1999.
S. Meyn and R.L. Tweedie. Markov Chains and Stochastic
Stability. Cambridge University Press, New York, NY,
USA, 2009.
P. Poupart, N. Vlassis, J. Hoey, and K. Regan. An analytic
solution to discrete bayesian reinforcement learning. In
ICML, 2006.
P. M. Samson. Concentration of measure inequalities for
Markov chains and φ-mixing processes. Annals of Probability, 28(1):416–461, 2000.
Y. Seldin, N. Cesa-Bianchi, F. Laviolette, P. Auer,
J. Shawe-Taylor, and J. Peters. PAC-Bayesian analysis of the exploration-exploitation trade-off. In On-line
Trading of Exploration and Exploitation 2, ICML-2011
workshop, 2011a.
Y. Seldin, F. Laviolette, J. Shawe-Taylor, J. Peters,
and P. Auer.
PAC-Bayesian analysis of martingales and multiarmed bandits.
Technical report,
http://arxiv.org/abs/1105.2416, 2011b.
J. Shawe-Taylor and R. C. Williamson. A PAC analysis of
a Bayesian estimator. In COLT, 1997.

to denote the functional over F which maps f ∈ F
to g(∆(f )). When g is measurable, this allows us to
write ρ0 g(∆), where ρ0 is a measure over F.
For any functional ∆ over F, by the convex duality of
relative entropy, we have

1
log ρ0 eλ∆ + K(ρ, ρ0 ) .
λ>0 λ

ρ∆ ≤ inf

We can apply this to ∆(f ) = (R(f ))2+ . Let λ be a
positive real to be chosen later. Then, we will see that
it will be sufficient to bound the right tail probabilities
of ρ0 eλ∆(f ) . By Markov’s inequality and Fubini, for
ε > 0,





P ρ0 eλ∆ ≥ ε ≤ ε−1 E ρ0 eλ∆ = ε−1 ρ0 E eλ∆ ,
where we used the boundedness of R. Now, fix some
f ∈ F. Then,
Z ∞ 
h
i

λ∆(f )
E e
= 1+
P eλ∆(f ) ≥ t dt
Z1 ∞
= 1+
P (λ∆(f ) ≥ t) et dt
0
r !
Z ∞
t
et dt
= 1+
P R(f ) ≥
λ
0
Z ∞
≤ 1+C
e−c(t/λ)+t dt
by (2)
0

=

where in the last
we have chosen λ = c−1. Hence,
 step
1+C(c−1)
λ∆
P ρ0 e ≥ ε ≤
. Let ε = (1 + C(c
ε
 − 1))/δ to
get P log ρ0 eλ∆ ≥ log((1 + C(c − 1))/δ) ≤ δ. Hence,

A. L. Strehl and M. L. Littman. A theoretical analysis of
model-based interval estimation. In ICML, 2005.

ρ∆ ≤

R. S. Sutton and A. G. Barto. Reinforcement Learning:
An Introduction. MIT Press, Cambridge, MA, 1998.
R. S. Sutton, H. R. Maei, D. Precup, S. Bhatnagar, D. Silver, Cs. Szepesvári, and E. Wiewiora. Fast gradientdescent methods for temporal-difference learning with
linear function approximation. In ICML, 2009.

C
= 1 + C(c − 1) ,
1+
c/λ − 1

log((1 + C(c − 1))/δ) + K(ρ, ρ0 )
c−1

holds w.p. 1 − δ. The proof is finished by noting that
q
p
ρ(R)+ ≤ ρ(R)2+ = ρ∆ .

L. G. Valiant. A theory of the learnable. Commun. ACM,
27(11):1134–1142, 1984.
T. Wang, D. Lizotte, M. Bowling, and D. Schuurmans.
Bayesian sparse sampling for on-line reward optimization. In ICML, 2005.

7
7.1

Appendix
Proof of Theorem 1

Proof. Let ∆ be some functional over F. For some
real-valued function over the reals g, we will use g(∆)

7.2

Proof of Lemma 2

In this section we give an extension of Bernstein’s inequality based on Samson (2000).
Let X1 , . . . , Xn be a time-homogeneous Markov chain
with transition kernel P (·|·) taking values in some measurable space X . We shall consider the concentration
of the average of the Hidden-Markov Process
(X1 , f (X1 )), . . . , (Xn , f (Xn )),

where f : X → [0, B] is a fixed measurable function.
To arrive at such an inequality, we need a characterization of how fast (Xi ) forgets its past.
For i > 0, let P i (·|x) be the i-step transition probability kernel: P i (A|x) = P (Xi+1 ∈ A | X1 = x) (for
all A ⊂ X measurable). Define the upper-triangular
matrix Γn = (γij ) ∈ Rn×n as follows:
2
γij
=

P j−i (·|x) − P j−i (·|y)

sup
(x,y)∈X 2

TV

.

(8)

The following result is a trivial corollary of Theorem 2
of Samson (2000) (Theorem 2 is stated for empirical
processes and can be considered as a generalization
of Talagrand’s inequality to dependent random variables):
Theorem 6. Let f be a measurable function on X
whose values lie in [0, B], (Xi )1≤i≤n be a homogeneous
Markov chain taking values in X and let Γn be the
matrix with elements defined by (8). Let
n

for 1 ≤ i < j ≤ n and let γii = 1 (1 ≤ i ≤ n).
Matrix Γn , and its operator norm kΓn k w.r.t. the Euclidean distance, are the measures of dependence for
the random sequence X1 , X2 , . . . , Xn . For example if
the Xi s are independent, Γn = I and kΓn k = 1. In
general kΓn k, which appears in the forthcoming concentration inequalities for dependent sequences, can
grow with n. Since the concentration bounds are ho2
2
mogeneous in n/ kΓn k , a larger value kΓn k means
a smaller “effective” sample size. This motivates the
following definition.
Definition 4. We say that a time-homogeneous
Markov chain uniformly quickly forgets its past if
2
τ = supn≥1 kΓn k < +∞. Further, τ is called the
forgetting time of the chain.
Conditions under which a Markov chain uniformly
quickly forgets its past are of major interest. The following proposition, extracted from the discussion on
pages 421–422 of the paper by Samson (2000), gives
such a condition.
Proposition 5. Let µ be some nonnegative measure
on X with nonzero mass µ0 . Let P i be the i-step transition kernel as defined above. Assume that there exists
some integer r such that for all x ∈ X and all measurable sets A,
P r (A|x) ≤ µ(A).
(9)
Then,

√
kΓn k ≤

2
1

1 − ρ 2r

,

where ρ = 1 − µ0 .
Meyn and Tweedie (2009) call homogeneous Markov
chains that satisfy the majorization condition (9) uniformly ergodic. We note in passing that there are other
cases when kΓn k is known to be independent of n.
Most notable, this holds when the Markov chain is contracting. The matrix Γn can also be defined for more
general dependent processes and such that the theorem below remains valid. With such a definition, kΓn k
can be shown to be bounded for general Φ-dependent
processes.

Z=

1X
f (Xi ).
n i=1

Then, for every ε ≥ 0,
P (Z − E [Z] ≥ ε) ≤ exp −

ε2 n
2

2B kΓn k (E [Z] + ε)
!
ε2 n
P (E [Z] − Z ≥ ε) ≤ exp −
.
2
2B kΓn k E [Z]

!
,

Lemma 2 is an immediate consequence of this theorem.

