Yutaka Kano
Div. Mathematical Science
Osaka University
Japan

Patrik O. Hoyer
HIIT Basic Research Unit
Dept. of Comp. Science
University of Helsinki
Finland

One of the main questions one can answer using this
kind of theoretical framework is: ‘Under what circumstances and in what way can one determine causal
structure on the basis of observational data alone?’.
In many cases it is impossible or too expensive to perform controlled experiments, and hence methods for
discovering likely causal relations from uncontrolled
data would be very valuable.
Existing discovery algorithms (Spirtes et al. 2000;
Pearl 2000) generally work in one of two settings. In
the case of discrete data, no functional form for the
dependencies is usually assumed. On the other hand,
when working with continuous variables, a lineargaussian approach is almost invariably taken.
In this paper, we show that when working with
continuous-valued data, a significant advantage can
be achieved by departing from the gaussianity assumption. While the linear-gaussian approach usually
only leads to a set of possible models, equivalent in
their conditional correlation structure, a linear-nongaussian setting allows the full causal model to be estimated, with no undetermined parameters.
The paper is structured as follows. First, in section 2,
we describe our assumptions on the data generating
process. These assumptions are essential for the application of our causal discovery method, detailed in
sections 3 through 5. Section 6 discusses how one can
test whether the found model seems plausible. In section 7 we empirically verify that our algorithm works
as stated. Conclusions and future research directions
are given in section 8.

2

LINEAR CAUSAL NETWORKS

Assume that we observe data generated from a process
with the following properties:
1. The observed variables xi , i = {1 . . . n} can be
arranged in a causal order, such that no later vari-

e4

e2

x4
0.2

e2

x2
1

x2

4

x1

e1

e3

-3

x3

e1

e1

e2

x1

x1

x2

2

x1

e1

x4

0.5

e4

x2

0.5

e2

x1

e1

2
-2

e3

-5

-5

e3

x3

x3

3

x2

e2

Figure 1: A few examples of data generating models satisfying our assumptions. For example, in the leftmost model, the data is generated by first drawing the ei independently from their respective non-gaussian
distributions, and subsequently setting (in this order) x4 = e4 , x2 = 0.2x4 + e2 , x1 = x4 + e1 , and x3 =
−2x2 − 5x1 + e3 . (Here, we have assumed for simplicity that all the ci are zero, but this may not be the case
in general.) Note that the variables are not causally sorted (reflecting the fact that we usually do not know the
causal ordering a priori), but that in each of the graphs they can be arranged in a causal order, as all graphs
are directed acyclic graphs. In this paper we show that the full causal structure, including all parameters, are
identifiable given a sufficient number of observed data vectors x.
able causes any earlier variable. We denote such
a causal order by k(i). That is, the generating
process is recursive (Bollen 1989), meaning it can
be represented graphically by a directed acyclic
graph (DAG) (Pearl 2000; Spirtes et al. 2000).
2. The value assigned to each variable xi is a linear
function of the values already assigned to the earlier variables, plus a ‘disturbance’ (noise) term ei ,
and plus an optional constant term ci , that is
X
xi =
bij xj + ei + ci .
(1)
k(j)<k(i)

3. The disturbances ei are all continuous random
variables with non-gaussian distributions of nonzero variances, and the ei are
Q independent of each
other, i.e. p(e1 , . . . , en ) = i pi (ei ).
A model with these three properties we call a Linear,
Non-Gaussian, Acyclic Model, abbreviated LiNGAM.
We assume that we are able to observe a large number
of data vectors x (which contain the components xi ),
and each is generated according to the above described
process, with the same causal order k(i), same coefficients bij , same constants ci , and the disturbances ei
sampled independently from the same distributions.
Note that the above assumptions imply that there are
no unobserved confounders (Pearl 2000). Spirtes et al.
(2000) call this the causally sufficient case. See figure 1 for a few examples of data models fulfilling the
assumptions of our model.
A key difference to most earlier work on the linear, causally sufficient, case is the assumption of nongaussianity of the disturbances. In most work, an explicit or implicit assumption of gaussianity has been

made (Bollen 1989; Spirtes et al. 2000). An assumption of gaussianity of disturbance variables makes the
full joint distribution over the xi gaussian, and the
covariance matrix of the data embodies all one could
possibly learn from observing the variables. Hence,
all conditional correlations can be computed from the
covariance matrix, and discovery algorithms based on
conditional independence can be easily applied.
However, it turns out that an assumption of nongaussianity may actually be more useful. In particular, it turns out that when this assumption is valid,
the complete causal structure can in fact be estimated,
without any prior information on a causal ordering of
the variables. This is in stark contrast to what can be
done in the gaussian case: algorithms based only on
second-order statistics (i.e. the covariance matrix) are
generally not able to discern the full causal structure
in most cases. The simplest such case is that of two
variables, x1 and x2 . A method based only on the covariance matrix has no way of preferring x1 → x2 over
the reverse model x1 ← x2 ; indeed the two are indistinguishable in terms of the covariance matrix (Spirtes
et al. 2000). However, assuming non-gaussianity, one
can actually discover the direction of causality, as first
shown by Shimizu and Kano (2003b). This result can
be extended to several variables (Shimizu et al. 2005).
Here, we further develop the method so as to estimate
the full linear model, including all parameters.

3

MODEL IDENTIFICATION
USING INDEPENDENT
COMPONENT ANALYSIS

The key to the solution to the linear discovery problem
is to realize that the observed variables are linear func-

tions of the disturbance variables, and the disturbance
variables are mutually independent and non-gaussian.
If we as preprocessing subtract out the mean of each
variable xi , we are left with the following system of
equations:
x = Bx + e,
(2)
where B is a matrix that could be permuted (by simultaneous equal row and column permutations) to
strict lower triangularity if one knew a causal ordering k(i) of the variables. (Strict lower triangularity is
here defined as lower triangular with all zeros on the
diagonal.) Solving for x one obtains
x = Ae,

(3)

where A = (I − B)−1 . Again, A could be permuted
to lower triangularity (although not strict lower triangularity, actually in this case all diagonal elements
will be non-zero) with an appropriate permutation
k(i). Taken together, equation (3) and the independence and non-gaussianity of the components of e define the standard linear independent component analysis model.
Independent component analysis (ICA) (Comon 1994;
Hyvärinen et al. 2001) is a fairly recent statistical technique for identifying a linear model such as that given
in equation (3). If the observed data is a linear, invertible mixture of non-gaussian independent components,
it can be shown (Comon 1994) that the mixing matrix
A is identifiable (up to scaling and permutation of the
columns, as discussed below) given enough observed
data vectors x. Furthermore, efficient algorithms for
estimating the mixing matrix are available (Hyvärinen
1999).
We again want to emphasize that ICA uses nongaussianity (that is, more than covariance information) to estimate the mixing matrix A (or equivalently
its inverse W = A−1 ). For gaussian disturbance variables ei , ICA cannot in general find the correct mixing
matrix because many different mixing matrices yield
the same covariance matrix, which in turn implies the
exact same gaussian joint density. Our requirement for
non-gaussianity of disturbance variables stems from
the same requirement in ICA.
While ICA is essentially able to estimate A (and W),
there are two important indeterminacies that ICA cannot solve: First and foremost, the order of the independent components is in no way defined or fixed.
Thus, we could reorder the independent components
and, correspondingly, the columns of A (and rows of
W) and get an equivalent ICA model (the same probability density for the data). In most applications of
ICA, this indeterminacy is of no significance and can
be ignored, but in LiNGAM, we can and we have to

find the correct permutation as described in section 4
below.
The second indeterminancy of ICA concerns the scaling of the independent components. In ICA, this is
usually handled by assuming all independent components to have unit variance, and scaling W and A
appropriately. On the other hand, in LiNGAM (as in
SEM) we allow the disturbance variables to have arbitrary (non-zero) variances, but fix their weight (connection strength) to their corresponding observed variable to unity. This requires us to re-normalize the rows
of W so that all the diagonal elements equal unity,
before computing B, as described in the LiNGAM algorithm.
Our discovery algorithm, detailed in the next section,
can be briefly summarized as follows: First, use a standard ICA algorithm to obtain an estimate of the mixing matrix A (or equivalently of W), and subsequently
permute it and normalize it appropriately before using it to compute B containing the sought connection
strengths bij .

4

LiNGAM DISCOVERY
ALGORITHM

Based on the observations given in sections 2 and 3,
we propose the following causal discovery algorithm:
LiNGAM discovery algorithm
1. Given an n×m data matrix X (n  m), where each
column contains one sample vector x, first subtract
the mean from each row of X, then apply an ICA
algorithm to obtain a decomposition X = AS where
S has the same size as X and contains in its rows
the independent components. From here on, we will
exclusively work with W = A−1 .
2. Find the one and only permutation of rows of W
f without any zeros on the
which yields a matrix W
main diagonal. In practice, small estimation errors
will cause all elements of W to be non-zero, and
hence the permutation is sought which minimizes
P
f
i 1/|Wii |.
f by its corresponding diagonal
3. Divide each row of W
f 0 with all ones on
element, to yield a new matrix W
the diagonal.
b of B using B
b =I−W
f 0.
4. Compute an estimate B
5. Finally, to find a causal order, find the permutation
matrix P (applied equally to both rows and columns)
b which yields a matrix B
e = PBP
b T which is as
of B

close as possible to strictly lower triangular.
This
P
e2 .
can be measured for instance using i≤j B
ij

to yield a diagonal with all ones, and then remove this
diagonal and flip the sign of the remaining coefficients,
as specified in steps 3 and 4.

A complete Matlab code package implementing this algorithm is available online at our LiNGAM homepage:

Although we now have estimates of all coefficients
bij we do not yet have available a causal ordering
k(i) of the variables. Such an ordering (in general
there may exist many if the generating network is not
fully connected) is important for visualizing the resulting graph, and also plays a role in our edge pruning
method. A causal ordering can be found by permuting
both rows and columns (using the same permutation)
b (containing the estimated connection
of the matrix B
strengths) to yield a strictly lower triangular matrix.
If the estimates were exact, this would be a trivial
task. However, since our estimates will not contain exact zeros, we will have to settle for approximate strict
lower triangularity, measured for instance as described
in step 5. Again, finding the best permutation can
be done brute-force for low dimensionalities (our current implementation), but for higher dimensionalities
a more sophisticated method is required. We hope to
have such a method implemented in our code package
by the time you read this.

http://www.cs.helsinki.fi/group/neuroinf/lingam/

We now describe each of these steps in more detail.
In the first step of the algorithm, the ICA decomposition of the data is computed. Here, any standard
ICA algorithm can be used. Although our implementation uses the FastICA algorithm (Hyvärinen 1999),
one could equally well use one of the many other algorithms available, see e.g. (Hyvärinen et al. 2001).
However, it is important to select an algorithm which
can estimate independent components of many different distributions, as in general the distributions of the
disturbance variables will not be known in advance.
Because of the permutation indeterminancy of ICA,
the rows of W will be in random order. This means
that we do not yet have the correct correspondence
between the disturbance variables ei and the observed
variables xi . The former correspond to the rows of
W while the latter correspond to the columns of W.
Thus, our first task is to permute the rows to obtain a
correspondence between the rows and columns. If W
were estimated exactly, there would be only a single
row permutation that would give a matrix with no
zeros on the diagonal, and this permutation gives the
correct correspondence. (A proof of this is given in
Appendix A.)
In practice, however, ICA algorithms applied on finite
data sets will yield estimates which are only approximately zero for those elements which should be exactly
zero. Thus, our algorithm searches for the permutation using a cost function which heavily penalizes small
absolute values in the diagonal, as specified in step 2.
In addition to being intuitively sensible, this cost function can also be derived from a maximum-likelihood
framework; for details, see Appendix B.
When the number of observed variables xi is relatively
small (less than eight or so) then finding the best permutation is easy, since a simple exhaustive search can
be performed. This is what our current implementation uses. For larger dimensionalities, however, this
quickly becomes infeasible. We are currently developing more efficient methods to tackle cases with tens of
variables or more. For up-to-date information, refer to
the LiNGAM webpage (see URL above).
Once we have obtained the correct correspondence between rows and columns of the ICA decomposition,
calculating our estimates of the bij is straightforward.
First, we normalize the rows of the permuted matrix

5

PRUNING EDGES

After finding a causal ordering k(i), we can set to zero
b which are implied zero by the
the coefficients of B
order (i.e. those corresponding to the upper triangular part of the causally permuted connection matrix
e However, all remaining connections are in general
B).
non-zero. Even estimated connection strengths which
are exceedingly weak (and hence probably zero in the
generating model) remain and the network is fully connected. Both for achieving an intuitive understanding
of the data, and especially for visualization purposes,
a pruned network would be desirable.
Fortunately, pruning edges once we know a causal ordering of the variables is a well-known problem, and
extensively discussed in the Structural Equation Modeling (SEM) tradition (Bollen 1989). Here, we propose
a basic method based on this correspondence. In our
implementation we take the causal ordering obtained
from the LiNGAM algorithm, and then simply estimate the connection strengths using covariance information alone for different resamplings of the original
data. In this way, it is possible to obtain measures
of the variances of the estimates of the bij , and use
these variances to prune those edges whose estimated
means are low compared with their standard deviations. Future versions of our software package should
incorporate the more advanced methods developed in

the SEM community, possibly taking into account the
non-gaussianity of the data as well (Shimizu and Kano
2003b).

6

TESTING THE ASSUMPTIONS

The LiNGAM algorithm consistently estimates the
connection strengths (and a causal order) if the model
assumptions hold and the amount of data is sufficient.
But what if our assumptions do not in fact hold? In
such a case there is of course no guarantee that the
proposed discovery algorithm will find true causal relationships between the variables.
The good news is that, in some cases, it is possible to
detect problems. If, for instance, the estimated mab cannot be permuted to yield anything close to
trix B
a strictly lower triangular matrix, one knows that one
or more of the assumptions do not hold and one thus
cannot count on the results. Thus, our current implementation reports how well the triangularity condition
holds, and warns the user if the end result is far from
triangular.
Another possible test is whether the components (rows
of S) are actually independent. If the ‘independent
components’ found by ICA are in fact not very independent, then the data did not arise from the assumed
model. Independence of continuous random variables
is a property which is more problematic to test than
the triangularity condition. Since the linear correlations are in fact forced to zero by many ICA algorithms
(including the one we use) it is of no use to test these.
Rather, tests based on some form of nonlinear correlations must be used (Murata 2001; Shimizu and Kano
2003a). Although not in our current first implementation, we hope to add such tests to our code package as
soon as possible.
Unfortunately, however, it is never possible to completely confirm the assumptions (and hence the found
causal model) purely from observational data. Experiments, where the individual variables are explicitly manipulated (often by random assignment) and
their effects monitored, are the only way to verify any
causal model. Nevertheless, it is fair to say that if
both the triangularity and the independence conditions hold then the found causal model is likely to
hold, at least approximately. Only pathological cases
constructed by mischievous data designers seem likely
to be problematic for our framework. Thus, we think
that a LiNGAM analysis will prove a useful first step
in many cases for providing educated guesses of causal
models, which might subsequently be verified in systematic experiments.

7

EXPERIMENTS

To verify the validity of our method (and of our Matlab code), we performed extensive experiments with
simulated data. All experimental code (including the
precise code to produce figure 2) is included in the
LiNGAM code package.
We repeatedly performed the following experiment:
1. First, we randomly constructed a strictly lowertriangular matrix B. Various dimensionalities (8
or less) were used. Both fully connected (no zeros
in the strictly lower triangular part) and sparse
networks (many zeros) were tested. We also randomly selected variances of the disturbance variables and values for the constants ci .
2. Next, we generated data by independently drawing the disturbance variables ei from gaussian distributions and subsequently passing them through
a power non-linearity (raising the absolute value
to an exponent in the interval [0.5, 0.8] or [1.2,
2.0], but keeping the original sign) to make them
non-gaussian. Various data set sizes were tested.
The ei were then scaled to yield the desired variances, and the observed data X was generated
according to the assumed recursive process.
3. Before feeding the data to the LiNGAM algorithm, we randomly permuted the rows of the data
matrix X to hide the causal order with which the
data was generated. At this point, we also permuted B, the ci , as well as the variances of the
disturbance variables to match the new order in
the data.
4. Finally, we fed the data to our discovery algorithm, and compared the estimated parameters
to the generating parameters. In particular, we
made a scatterplot of the entries in the estimated
b against the corresponding ones in B.
matrix B
Since the number of different possible parameter configurations is limitless, we feel that the reader is best
convinced by personally running the simulations using
various settings. This can be easily done by anyone
with access to Matlab.1 Nevertheless, we here show
some representative results.
Figure 2 gives combined scatterplots of the elements of
b versus those of B. The different plots correspond to
B
different dimensionalities (numbers of variables) and
different data sizes (numbers of data vectors), where
1

Note that a version running on the freely available Octave software is currently in development and might be
available by the time you read this.

2

2

0

0

0

2

2

2

2

5

7

0

2

2

0

2

2

2

2

0

0

0

2

2

2

2

estimated b ij

number of variables

3

2

0

2

2

0

2

2

2

2

0

0

0

2

2

2

2

0

2

2

0

2

2

0

2

2

0

2

2

0

2

generating b ij

100

1000

10000

number of data vectors
Figure 2: Scatterplots of the estimated bij versus the original (generating) values. The different plots correspond
to different numbers of variables and different numbers of data vectors. Although for small data sizes the
estimation often fails, when there is sufficient data the estimation works essentially flawlessly, as evidenced by
the grouping of the points along the diagonal.
each plot combines the data for a number of different network sparseness levels and non-linearities. Although for very small data sizes the estimation often
fails, when the data size grows the estimation works
practically flawlessly, as evidenced by the grouping of
the datapoints onto the main diagonal.
In summary, the experiments verify the correctness of
the method, and demonstrate that reliable estimation
is possible even with fairly limited amounts of data.

8

CONCLUSIONS AND
FUTURE WORK

Developing methods for causal inference from nonexperimental data is a fundamental problem with a
very large number of potential applications. Although
one can never fully prove the validity of a causal model
from observational data alone, such methods are nevertheless crucial in cases where it is impossible or very
costly to perform experiments.
Previous methods developed for linear causal models
(Bollen 1989; Spirtes et al. 2000; Pearl 2000) have

been based on an explicit or implicit assumption of
gaussianity, and have hence been based solely on the
covariance structure of the data. Because of this, additional information (such as the time-order of the
variables) is usually required to obtain a full causal
model of the variables. Without such information, algorithms based on the gaussianity assumption cannot
in most cases distinguish between multiple equally possible causal models.
In this paper, we have shown that an assumption of
non-gaussianity of the disturbance variables, together
with the assumption of linearity and causal sufficiency,
allows the causal model to be completely identified.
Furthermore, we have provided a practical algorithm
which estimates the causal structure under these assumptions.
Future work will focus on implementational issues in
problems involving tens of variables or more. It remains an open question what algorithms are best
suited for optimizing our objective functions, which
measure the goodness of the permutations, in such
cases. Further, the practical value of the LiNGAM
analysis needs to be determined by applying it to real-

world datasets and comparing it to other methods for
causal inference from non-experimental data. In many
cases involving real-world data, practitioners in the
field already have a fairly good understanding of the
causal processes underlying the data. An interesting
question is how well methods such as ours do on such
datasets. For the most recent developments, please see
the webpage:
http://www.cs.helsinki.fi/group/neuroinf/lingam/

Acknowledgements
The authors would like to thank Alex Pothen and
Heikki Mannila for discussions relating to algorithms
for solving the permutation problems.
S.S. was
supported by Grant-in-Aid for Scientific Research
from the Ministry of Education, Culture and Sports,
Japan. A.H. was supported by the Academy of Finland through an Academy Research Fellow Position
and project #203344. P.O.H. was supported by the
Academy of Finland project #204826.
References
Bollen, K. A. (1989). Structural Equations with Latent Variables. John Wiley & Sons.
Comon, P. (1994). Independent component analysis
– a new concept? Signal Processing 36, 287–314.
Hyvärinen, A. (1999). Fast and robust fixed-point
algorithms for independent component analysis.
IEEE Trans. on Neural Networks 10 (3), 626–
634.
Hyvärinen, A., J. Karhunen, and E. Oja (2001).
Independent Component Analysis. Wiley Interscience.
Murata, N. (2001). Properties of the empirical characteristic function and its application to testing for independence. In Proc. 3rd International
Conference on Independent Component Analysis
and Blind Signal Separation, pp. 445–450.
Pearl, J. (2000). Causality: Models, Reasoning, and
Inference. Cambridge University Press.
Shimizu, S., A. Hyvärinen, P. Hoyer, and Y. Kano
(2005). Finding a causal ordering via independent component analysis. Computational Statistics & Data Analysis. In press.
Shimizu, S. and Y. Kano (2003a). Examination of
independence in independent component analysis. In New Developments in Psychometrics
(Proc. IMPS2001), pp. 665–672.
Shimizu, S. and Y. Kano (2003b). Nonnormal structural equation modeling. Submitted.

Spirtes, P., C. Glymour, and R. Scheines (2000).
Causation, Prediction, and Search. MIT Press.

A

PROOF OF UNIQUENESS OF
ROW PERMUTATION

Here, we show that, were the estimates of ICA exact,
there is only a single permutation of the rows of W
which results in a diagonal with no zero entries.
It is well-known (Bollen 1989) that the DAG structure of the network guarantees that for some permutation of the variables, the matrix B is strictly lowerf (where
triangular. This implies that the correct W
the disturbance variables are aligned with the observed
variables) can be permuted to lower-triangular form
(with no zero entries on the diagonal) by equal row
and column permutations, i.e.
f = Pd MPT ,
W
d

(4)

where M is lower-triangular and has no zero entries
on the diagonal, and Pd is a permutation matrix representing a causal ordering of the variables. Now, ICA
returns a matrix with randomly permuted rows,
f = P Pd MPT = P1 MPT ,
W = Pica W
d
2
ica

(5)

where Pica is the random ICA row permutation, and
on the right we have denoted by P1 = Pica Pd and
P2 = Pd , respectively, the row and column permutations from the lower triangular matrix M.
We now prove that W has no zero entries on the diagonal if and only if the row and column permutations
are equal, i.e. P1 = P2 . Hence, there is only one
row permutation of W which yields no zero entries on
the diagonal, and it is the one which finds the correspondence between the disturbance variables and the
observed variables.
Lemma 1 Assume M is lower triangular and all diagonal elements are nonzero. A permutation of rows
and columns of M has only non-zero entries in the diagonal if and only if the row and column permutations
are equal.
Proof: First, we prove that if the row and columns
permutations are not equal, there will be zero elements
in the diagonal.
Denote by K a lower triangular matrix of all ones in
the lower triangular part. Denote by P1 and P2 two
permutation matrices. The number of non-zero diagonal entries in a permuted version of K is tr(P1 KPT2 ).
This is the maximum number of non-zero diagonal entries when an arbitrary lower triangular matrix is permuted.

We have tr(P1 KPT2 ) = tr(KPT2 P1 ). Thus, we
first consider permutations of columns only, given by
PT2 P1 . Assume the columns of K are permuted so that
the permutation is not equal to identity. Then, there
exists an index i so that the column of index i has been
moved to column index j where j < i (If there were
no such columns, all the columns would be moved to
the right, which is impossible.) Obviously, the diagonal entry in the j-th column in the permuted matrix is
zero. Thus, any column permutation not equal to the
identity creates at least one zero entry in the diagonal.
Thus, to have nonzero diagonal, we must have PT2 P1 =
I. This means that the column and row permutations
must be equal.
Next, assume that the row and column permutations
are equal. Consider M = I as a worst-case scenario.
Then the permuted matrix equals P1 IPT2 which equals
identity, and all the diagonal elements are nonzero.
Adding more nonzero elements in the matrix only increases the number of nonzero elements in the permuted version.
Thus, the lemma is proven.

B

ML DERIVATION OF
OBJECTIVE FUNCTION FOR
FINDING THE CORRECT ROW
PERMUTATION

Since the ICA estimates are never exact, all elements
of W will be non-zero, and one cannot base the permutation on exact zeros. Here we show that the objective

function for step 2 of the LiNGAM algorithm can be
derived from a maximum likelihood framework.
Let us denote by eit the value of disturbance variable i
for the t:th datavector of the dataset. Assume that we
model the disturbance variables eit by a generalized
gaussian density:
log p(eit ) = −|eit |α /β + Z

(6)

where the α, β are parameters and Z is a normalization
constant. Then, the log-likelihood of the model equals
XX
t

i

−

eit
βwii

α

=−

X
i

X
1
|eit |α
α
β|wii | t

(7)

because each row of W is subsequently divided by its
diagonal element. To maximize the likelihood, we find
the permutation of rows for which the diagonal elements maximize this term. For simplicity, assuming
that the pdf’s of all independent components are the
same, this means we solve
min
all row perms

X
i

1
|wii |α

(8)

In principle, we could estimate α from the data using
ML estimation as well, but for simplicity we fix it to
unity because it does not really change the qualitative
behaviour of the objective function. Regardless of its
value, this objective function heavily penalizes small
values on the diagonal, as we intuitively (based on the
argumentation in section 4) require.

